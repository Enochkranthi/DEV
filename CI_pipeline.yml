# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger: none

pool:
  vmImage: ubuntu-latest

variables:
- group: 'DEV'

steps:

- script: pip install databricks-cli
  displayName: 'Install Databricks cli'

- task: AzureCLI@2
  displayName: 'Install Azure ML CLI '
  inputs:
    azureSubscription: Azure service connection
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az extension add -n azure-cli-ml'
- task: AzureCLI@2
  displayName: 'Update Azure ML CLI '
  inputs:
    azureSubscription: Azure service connection
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: 'az extension update --name azure-cli-ml'
    
- script: |
    echo "$(databricksHost)
    $(DB_PAT)" | databricks configure --token
  displayName: 'configure databricks cli'

- script: databricks workspace ls
  displayName: 'test databricks cli'

- script: databricks repos delete --path /Repos/prakashpoorna998@gmail.com/DEV
  displayName: 'Deleting old repo'

- script: databricks repos create --url https://github.com/Enochkranthi/DEV.git --provider gitHub --path /Repos/prakashpoorna998@gmail.com/DEV
  displayName: 'Importing git repo'
    
- script: |
      json_path=$(Build.SourcesDirectory)/MLOps/dev-cluster.json
      if databricks clusters get --cluster-name Small 2>/dev/null; then
        echo "Cluster exists"
      else 
        databricks clusters create --json-file "$json_path"
      fi
      cluster_id=$(databricks clusters get --cluster-name Small | jq -r '.cluster_id')
  displayName: 'Create Cluster'

- script: |
      while true; do
        cluster_status=$(databricks clusters get --cluster-name Small | jq -r '.state')
        if [[ $cluster_status == "RUNNING" ]]; then
          break
        fi
        sleep 10
      done
  displayName: 'Wait till cluster Starts'

- script: |
    
    cluster_id=$(databricks clusters get --cluster-name Small | jq -r '.cluster_id')
    notebook_path="/Repos/prakashpoorna998@gmail.com/DEV/DataScience/train"
    databricks jobs configure --version=2.1
    jobId=$(databricks runs submit --json '{
      "existing_cluster_id": "'$cluster_id'",
      "notebook_task": {
      "notebook_path": "'$notebook_path'"
      }
    }' | jq -r '.run_id')
    #echo "##vso[task.setvariable variable=jobId]$jobId"
    echo "Job ID: $jobId"
  displayName: 'Training the model'

- bash: 'mkdir metadata'
  displayName: 'Make Metadata'

- task: AzureCLI@2
  displayName: 'Registering the model'
  inputs:
    azureSubscription: Azure service connection
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      az ml model register -g $(azureml.resourceGroup) -w $(azureml.workspaceName) -n $(model.name) -f metadata/run.json --model-path Model/model.pkl -d "Regression" --tag "data"="Salary_prediction" --tag "model"="Regression" --model-framework spark  -t metadata/model.json


- task: CopyFiles@2
  displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)'
    Contents: |
     **/Data Science/*
     **/Output/*
     **/Model/*
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishPipelineArtifact@1
  displayName: 'Publish Pipeline Artifact'
  inputs:
    targetPath: '$(Build.ArtifactStagingDirectory)'
    artifact: Artifacts



