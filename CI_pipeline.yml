# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger: none

pool:
  vmImage: ubuntu-latest

variables:
- group: 'DEV'

steps:
- script: echo Hello, world!
  displayName: 'Run a one-line script'

- script: |
    echo See https://aka.ms/yaml
  displayName: 'Run a multi-line script'

- script: pip install databricks-cli
  displayName: 'Install Databricks cli'

- script: |
    echo "$(databricksHost)
    $(DB_PAT)" | databricks configure --token
  displayName: 'configure databricks cli'

- script: databricks workspace ls
  displayName: 'test databricks cli'

- script: databricks repos delete --path /Repos/prakashpoorna998@gmail.com/DEV
  displayName: 'Deleting old repo'

- script: databricks repos create --url https://github.com/Enochkranthi/DEV.git --provider gitHub --path /Repos/prakashpoorna998@gmail.com/DEV
  displayName: 'Importing git repo'
    
- script: |
      json_path=$(Build.SourcesDirectory)/MLOps/dev-cluster.json
      if databricks clusters get --cluster-name Small 2>/dev/null; then
        echo "Cluster exists"
      else 
        databricks clusters create --json-file "$json_path"
      fi
      cluster_id=$(databricks clusters get --cluster-name Small | jq -r '.cluster_id')
  displayName: 'Create Cluster'

- script: |
      while true; do
        cluster_status=$(databricks clusters get --cluster-name Small | jq -r '.state')
        if [[ $cluster_status == "RUNNING" ]]; then
          break
        fi
        sleep 10
      done
  displayName: 'Wait till cluster Starts'

- script: |
    
    cluster_id=$(databricks clusters get --cluster-name Small | jq -r '.cluster_id')
    notebook_path="/Repos/prakashpoorna998@gmail.com/DEV/Data Science/train"
    databricks jobs configure --version=2.1
    jobId=$(databricks runs submit --json '{
      "existing_cluster_id": "'$cluster_id'",
      "notebook_task": {
      "notebook_path": "'$notebook_path'"
      }
    }' | jq -r '.run_id')
    #echo "##vso[task.setvariable variable=jobId]$jobId"
    echo "Job ID: $jobId"

- bash: 'mkdir metadata && mkdir Outputs'
  displayName: 'Make Metadata and Output directory'


- task: CopyFiles@2
  displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)'
    Contents: |
     **/Data Science/*
     **/Output/*
     **/Model/*
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishPipelineArtifact@1
  displayName: 'Publish Pipeline Artifact'
  inputs:
    targetPath: '$(Build.ArtifactStagingDirectory)'
    artifact: landing
